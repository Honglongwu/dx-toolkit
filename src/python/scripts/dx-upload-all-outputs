#!/usr/bin/env python
#
# Copyright (C) 2013-2014 DNAnexus, Inc.
#
# This file is part of dx-toolkit (DNAnexus platform client libraries).
#
#   Licensed under the Apache License, Version 2.0 (the "License"); you may not
#   use this file except in compliance with the License. You may obtain a copy
#   of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#   WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#   License for the specific language governing permissions and limitations
#   under the License.

import os, sys, json, argparse
import pprint
import dxpy

## contains specific helper functions
from dxpy.utils import file_load_utils


''' This helper script uploads all output files from the virtual machine
running in the cloud to the cloud filesystem (S3).

Specification

Part 1: 
   Upload everything that is in the output directory, and generate a
   $HOME/job_output.json file that describes it. 
   (a)
      Figure out what exists in the output directory, and is relevant. 
      The relevant formats are: 
      <odir>/xxx/yyy
          xxx == key
          yyy == file name

   (b) Upload everything that is in the output directory
         fasta_gz = dxpy.upload_local_file("%s.fa.gz" % name);

   (c) Generate a $HOME/job_output.json file that describes it. 

Part 2:
   If there is an output spec, compare against it. 
'''
pp = pprint.PrettyPrinter(indent=4)

# Extract the outputSpec, if it exists
def get_output_spec():
    output_spec = None
    if 'DX_JOB_ID' in os.environ:
        # this ins't going to work because 
        # look at the API for app-describe, job-describe
        # get the ID of the executable and describe that
        print("found the job id");
        job = dxpy.api.job_describe(dxpy.JOB_ID)
        output_spec = job.outputSpec  ## what if there is no such field? 
    elif 'DX_TEST_DXAPP_JSON' in os.environ:
        path_to_dxapp_json = os.environ['DX_TEST_DXAPP_JSON']
        with open(path_to_dxapp_json, 'r') as fd:
            dxapp_json = json.load(fd)
            output_spec = dxapp_json.get('outputSpec')
        
    # convert to a dictionary. Each record in the output spec
    # has {name, class} attributes. 
    rc = {}
    if output_spec == None:
        return rc
    else:
        for spec in output_spec:
            rc[spec['name']] = spec['class']
        return rc

'''
Traverse an output directory, and return all the files in
it. Skip directory, for sanity. 
'''
def traverse_output_subdir(odir, subdir):
    files = []  
    for fname in os.listdir(subdir):
        path = os.path.join(odir, fname)
        if os.path.isdir(path):
            continue
        if path.startswith('.'):
            continue
        files.append(fname)
    return files

'''
figure out what the output directory looks like. 
We are interested in elements of the form:
     <odir>/xxx/yyy
         xxx == key
         yyy == file name
 
     Arrays look like this:
       <odir>/xxx/yyy
       <odir>/xxx/vvv
       <odir>/xxx/zzz
'''
def analyze_output_dir():
    # figure out the subdirs, these map to keys
    odir = file_load_utils.calc_output_dir()
    if not os.path.isdir(odir):
        return {}
    l = os.listdir(odir)
    subdirs = {}    # mapping from name to attributes
    for fname in l:
        path = os.path.join(odir, fname)
        if not os.path.isdir(path):
            continue
        if path.startswith('.'):
            continue
        rec = {'path' : path,
               'files' : traverse_output_subdir(odir, path),
               'dx_links' : []}
        subdirs[fname] = rec
    return subdirs
    
'''
Compare the records found in the output directory to the
output specification

An output spec is a list of records with {class, name} attributes.
'''
def compare_to_output_spec(recs, output_spec):
    # check all the output fields have been generated
    for key in output_spec:
        if not key in recs:
            raise dxpy.AppError("output key {} not generated".format(key))

    # annotate with classes, and sanity check
    for key in output_spec:
        clazz = output_spec[key]
        rec = recs[key]
        rec['class'] = clazz
        num_files = len(rec['files'])
        if clazz == 'file':
            if not num_files == 1:
                raise dxpy.AppError("key {} is of class {} but there are {} files".format(key, clazz, num_files))


'''
(b)
 Upload everything that is in the output directory
Add references to the uploaded objects into the records.

note: currently, files are uploaded sequentially, not in 
parallel. This could be improved in the future.
'''
def upload_all(subdirs):
    for key in subdirs:
        rec = subdirs[key]
        for fname in rec['files']:
            ## Upload the file
            path = os.path.join(rec['path'], fname)
            #pp.pprint("uploading file: " + path)
            f_obj = dxpy.upload_local_file(path)
            rec['dx_links'].append(dxpy.dxlink(f_obj))
    return subdirs

'''
update the output json file.
'''
def update_output_json(subdirs):
    # load existing file, if it exists
    output_json = {}
    output_file = file_load_utils.calc_output_json()
    if os.path.exists(output_file):
        with open(output_file, 'r') as fh:
            output_json = json.loads(fh.read())

    # add one record to the json output file
    def add_rec_to_json(key, clazz, dxlinks):
        if not key in output_json:
            if clazz == 'array:file':
                ## array type
                output_json[key] = dxlinks
            elif not len(dxlinks) == 1:
                output_json[key] = dxlinks
            else:
                ## singleton
                output_json[key] = dxlinks[0]
        else:
            if isinstance(output_json[key], list):
                output_json[key].extend(dxlinks)
            else:
                raise dxpy.AppError("Key " + key + " was found in output but is not an array")

    # add all the records
    for key in subdirs:
        rec = subdirs[key]
        dxlinks = rec['dx_links']
        if len(dxlinks) == 0:
            continue
        clazz = None
        if 'class' in rec:
            clazz = rec['class']
        add_rec_to_json(key, clazz, dxlinks)
    
    # write it back out
    #pp.pprint(output_json)
    with open(output_file, 'w') as fh:
        fh.write(json.dumps(output_json, indent=4) + '\n')

## entry point
output_spec = get_output_spec()
recs = analyze_output_dir()
print "found ", len(recs), " records"

# compare against the output spec
compare_to_output_spec(recs, output_spec)

#pp.pprint(recs)
recs = upload_all(recs)
update_output_json(recs)



